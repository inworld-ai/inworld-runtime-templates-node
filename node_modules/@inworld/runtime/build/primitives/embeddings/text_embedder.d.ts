import type { EmbedBatchResponse, EmbedResponse, TextEmbedderCreationConfig } from '../../common/api/library';
/**
 * Text Embedder class.
 * Generates vector embeddings from text for semantic processing.
 *
 * @example
 * ```typescript
 * // Create embedder instance
 * const embedder = await TextEmbedder.create({
 *   localConfig: {
 *     modelPath: 'models/embeddings/all-MiniLM-L6-v2',
 *     device: { type: 'CPU', index: 0 }
 *   }
 * });
 *
 * // Generate single embedding
 * const embedResponse = await embedder.embed("Hello world");
 * console.log(embedResponse.embedding); // [0.123, -0.456, ...]
 *
 * // Generate batch embeddings
 * const embedBatchResponse = await embedder.embedBatch(["text1", "text2", "text3"]);
 * console.log(embedBatchResponse.embeddings); // Array of embeddings
 * ```
 */
export declare class TextEmbedder {
    private embedderInterface;
    /**
     * Creates a new TextEmbedder instance (internal constructor).
     * Use `TextEmbedder.create()` static method instead.
     *
     * @param {any} embedderInterface - Addon TextEmbedderInterface instance
     * @internal
     */
    constructor(embedderInterface: any);
    /**
     * Creates a new TextEmbedder instance with the specified configuration.
     *
     * @param {TextEmbedderCreationConfig} config - Embedder creation configuration
     * @returns {Promise<TextEmbedder>} Promise resolving to TextEmbedder instance
     * @throws {EmbeddingError} If embedder creation fails
     *
     * @example
     * ```typescript
     * // Local model
     * const embedder = await TextEmbedder.create({
     *   local_config: {
     *     model_path: 'models/embeddings/all-MiniLM-L6-v2',
     *     device: { type: 'CPU', index: 0 }
     *   }
     * });
     *
     * // Remote API
     * const embedder = await TextEmbedder.create({
     *   remoteConfig: {
     *     provider: 'openai',
     *     modelName: 'text-embedding-ada-002',
     *     apiKey: process.env.INWORLD_API_KEY
     *   }
     * });
     * ```
     */
    static create(config: TextEmbedderCreationConfig): Promise<TextEmbedder>;
    /**
     * Generates an embedding vector for a single text string.
     *
     * @param {string} text - Text to convert to embedding
     * @returns {Promise<EmbedResponse>} Promise resolving to embedding with values array
     * @throws {EmbeddingError} If embedding generation fails
     *
     * @example
     * ```typescript
     * const embedResponse = await embedder.embed("Hello, world!");
     * console.log(embedResponse.embedding); // [0.123, -0.456, 0.789, ...]
     * console.log(embedResponse.embedding.length); // e.g., 384 for all-MiniLM-L6-v2
     * ```
     */
    embed(text: string): Promise<EmbedResponse>;
    /**
     * Generates embedding vectors for multiple text strings in a batch.
     * This is more efficient than calling `embed()` multiple times.
     *
     * @param {string[]} texts - Array of text strings to convert to embeddings
     * @returns {Promise<EmbedBatchResponse>} Promise resolving to batch of embeddings
     * @throws {EmbeddingError} If batch embedding generation fails
     *
     * @example
     * ```typescript
     * const embedBatchResponse = await embedder.embedBatch([
     *   "First text",
     *   "Second text",
     *   "Third text"
     * ]);
     *
     * embedBatchResponse.embeddings.forEach((embedding, i) => {
     *   console.log(`Embedding ${i}:`, embedding.slice(0, 5));
     * });
     * ```
     */
    embedBatch(texts: string[]): Promise<EmbedBatchResponse>;
    /**
     * Returns a simple array of numbers from an embedding vector.
     * This is a convenience method for working with embedding values.
     *
     * @param {EmbedResponse} embedResponse - EmbedResponse object
     * @returns {number[]} Array of embedding values
     *
     * @example
     * ```typescript
     * const embedding = await embedder.embed("Hello");
     * const values = TextEmbedder.toArray(embedding);
     * // Same as: const values = embedding.embedding;
     * ```
     */
    static toArray(embedResponse: EmbedResponse): number[];
}
