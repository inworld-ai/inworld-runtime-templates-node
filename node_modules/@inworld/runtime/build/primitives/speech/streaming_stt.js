"use strict";
/**
 * Streaming Speech-to-Text (StreamingSTT) primitive using N-API addon.
 * Handles real-time speech recognition from continuous audio streams.
 */
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __await = (this && this.__await) || function (v) { return this instanceof __await ? (this.v = v, this) : new __await(v); }
var __asyncGenerator = (this && this.__asyncGenerator) || function (thisArg, _arguments, generator) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var g = generator.apply(thisArg, _arguments || []), i, q = [];
    return i = Object.create((typeof AsyncIterator === "function" ? AsyncIterator : Object).prototype), verb("next"), verb("throw"), verb("return", awaitReturn), i[Symbol.asyncIterator] = function () { return this; }, i;
    function awaitReturn(f) { return function (v) { return Promise.resolve(v).then(f, reject); }; }
    function verb(n, f) { if (g[n]) { i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; if (f) i[n] = f(i[n]); } }
    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }
    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }
    function fulfill(value) { resume("next", value); }
    function reject(value) { resume("throw", value); }
    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.StreamingSTT = void 0;
const node_api_1 = require("../../internal/node_api");
const pb_helpers_1 = require("../../internal/pb_helpers");
const errors_1 = require("../errors");
/**
 * Streaming Speech-to-Text (StreamingSTT) class.
 * Converts continuous audio streams into real-time text transcriptions.
 * Supports both local models and cloud-based APIs with automatic chunking.
 *
 * @example
 * ```typescript
 * // Create StreamingSTT instance with cloud API
 * const streamingSTT = await StreamingSTT.create({
 *   remoteConfig: {
 *     apiKey: process.env.INWORLD_API_KEY,
 *     defaultConfig: {
 *       languageCode: 'en-US',
 *       silenceThresholdMs: 1500
 *     },
 *     defaultTimeout: { seconds: 30 }
 *   }
 * });
 *
 * // Create audio stream generator
 * async function* audioStreamGenerator() {
 *   for (const audioChunk of audioChunks) {
 *     yield audioChunk;
 *   }
 * }
 *
 * // Recognize speech from stream
 * const textStream = await streamingSTT.recognizeSpeechStream(
 *   audioStreamGenerator()
 * );
 *
 * for await (const transcription of textStream) {
 *   console.log('Transcription:', transcription);
 * }
 * ```
 */
class StreamingSTT {
    /**
     * Creates a new StreamingSTT instance (internal constructor).
     * Use `StreamingSTT.create()` static method instead.
     *
     * @param {any} streamingSTTInterface - Addon StreamingSTTInterface instance
     * @internal
     */
    constructor(streamingSTTInterface) {
        this.streamingSTTInterface = streamingSTTInterface;
    }
    /**
     * Creates a new StreamingSTT instance with the specified configuration.
     *
     * @param {StreamingSTTCreationConfig} config - StreamingSTT creation configuration
     * @returns {Promise<StreamingSTT>} Promise resolving to StreamingSTT instance
     * @throws {STTError} If StreamingSTT creation fails
     *
     * @example
     * ```typescript
     * // Local model
     * const streamingSTT = await StreamingSTT.create({
     *   localConfig: {
     *     sttModelPath: 'models/stt/whisper-base',
     *     vadModelPath: 'models/vad/silero_vad.onnx',
     *     sttDevice: { type: 'CPU', index: 0 },
     *     vadDevice: { type: 'CPU', index: 0 },
     *     defaultConfig: {
     *       languageCode: 'en-US',
     *       silenceThresholdMs: 1500
     *     }
     *   }
     * });
     *
     * // Cloud API
     * const streamingSTT = await StreamingSTT.create({
     *   remoteConfig: {
     *     apiKey: process.env.INWORLD_API_KEY,
     *     defaultConfig: {
     *       languageCode: 'en-US',
     *       silenceThresholdMs: 1500
     *     },
     *     defaultTimeout: { seconds: 30 }
     *   }
     * });
     * ```
     */
    static async create(config) {
        try {
            const env = (0, node_api_1.getInworldAddonEnv)();
            const speech = env.speech;
            if (!speech) {
                throw new errors_1.STTError('Speech namespace not available in addon environment');
            }
            const factory = speech.createStreamingSTTFactory();
            const configPb = pb_helpers_1.PbHelper.Speech.createStreamingSTTCreationConfig(config);
            const streamingSTTInterface = await factory.createStreamingSTT(configPb);
            return new StreamingSTT(streamingSTTInterface);
        }
        catch (error) {
            if (error instanceof errors_1.STTError) {
                throw error;
            }
            throw new errors_1.STTError(`Failed to create StreamingSTT instance: ${error.message}`, error.stack);
        }
    }
    /**
     * Recognizes speech from a continuous audio stream.
     * The audio stream should yield audio chunks as they become available.
     * Returns a stream of transcriptions that update as speech is detected.
     *
     * @param {AsyncIterable<AudioFrame>} audioStream - Stream of audio chunks
     * @param {StreamRecognizeSpeechRequestOptions} [options] - Optional recognition overrides
     * @returns {Promise<AsyncIterable<string>>} Stream of text transcriptions
     * @throws {STTError} If speech recognition fails
     *
     * @example
     * ```typescript
     * // Create audio stream generator
     * async function* audioStreamGenerator() {
     *   for (const chunk of audioChunks) {
     *     yield {
     *       data: chunk.data,      // Float32Array
     *       sampleRate: 16000
     *     };
     *   }
     * }
     *
     * const textStream = await streamingSTT.recognizeSpeechStream(
     *   audioStreamGenerator()
     * );
     *
     * for await (const transcription of textStream) {
     *   console.log('Transcription:', transcription);
     * }
     * ```
     */
    async recognizeSpeechStream(audioStream, options) {
        const session = await this.startRecognizeSpeechSession(audioStream, options);
        return this.transformSessionToText(session);
    }
    /**
     * Starts a streaming recognition session and exposes the underlying
     * transcription iterator together with a cancellation hook.
     *
     * @param {AsyncIterable<AudioFrame>} audioStream - Audio chunk stream
     * @param {StreamRecognizeSpeechRequestOptions} [options] - Optional overrides
     * @returns {Promise<StreamingSTTSession>} Streaming session
     */
    async startRecognizeSpeechSession(audioStream, options) {
        const { responseStream, bidirectionalStream } = await this.createStreamingSession(audioStream, options);
        return new StreamingSTTSessionImpl(this.transformPbStreamToResult(responseStream), bidirectionalStream);
    }
    /**
     * Transforms audio stream to protobuf audio stream.
     * @param {AsyncIterable<AudioFrame>} audioStream - Audio chunk stream
     * @returns {AsyncIterable<any>} Protobuf audio chunk stream
     * @private
     */
    transformAudioStreamToPb(audioStream) {
        return __asyncGenerator(this, arguments, function* transformAudioStreamToPb_1() {
            var _a, e_1, _b, _c;
            try {
                try {
                    for (var _d = true, audioStream_1 = __asyncValues(audioStream), audioStream_1_1; audioStream_1_1 = yield __await(audioStream_1.next()), _a = audioStream_1_1.done, !_a; _d = true) {
                        _c = audioStream_1_1.value;
                        _d = false;
                        const audioChunk = _c;
                        if (!audioChunk.data || audioChunk.data.length === 0) {
                            continue; // Skip empty chunks
                        }
                        if (!audioChunk.sampleRate || audioChunk.sampleRate <= 0) {
                            throw new errors_1.STTError('Valid sample rate is required for each audio chunk');
                        }
                        const pbChunk = pb_helpers_1.PbHelper.Speech.createAudioFrame(audioChunk);
                        yield yield __await(pbChunk);
                    }
                }
                catch (e_1_1) { e_1 = { error: e_1_1 }; }
                finally {
                    try {
                        if (!_d && !_a && (_b = audioStream_1.return)) yield __await(_b.call(audioStream_1));
                    }
                    finally { if (e_1) throw e_1.error; }
                }
            }
            catch (error) {
                throw new errors_1.STTError(`Failed to transform audio stream: ${error.message}`, error.stack);
            }
        });
    }
    /**
     * Pipes protobuf audio frames into the bidirectional stream.
     * @param {AsyncIterable<any>} pbAudioStream - Audio frames encoded as protobuf
     * @param {any} bidirectionalStream - Bidirectional stream wrapper from addon
     * @returns {Promise<void>} Resolves when audio transmission is complete
     * @private
     */
    async pipeAudioStreamToBidirectionalStream(pbAudioStream, bidirectionalStream) {
        var _a, e_2, _b, _c;
        try {
            try {
                for (var _d = true, pbAudioStream_1 = __asyncValues(pbAudioStream), pbAudioStream_1_1; pbAudioStream_1_1 = await pbAudioStream_1.next(), _a = pbAudioStream_1_1.done, !_a; _d = true) {
                    _c = pbAudioStream_1_1.value;
                    _d = false;
                    const audioChunk = _c;
                    const success = await bidirectionalStream.write(audioChunk);
                    if (success === false) {
                        throw new errors_1.STTError('Failed to write audio chunk to streaming STT interface');
                    }
                }
            }
            catch (e_2_1) { e_2 = { error: e_2_1 }; }
            finally {
                try {
                    if (!_d && !_a && (_b = pbAudioStream_1.return)) await _b.call(pbAudioStream_1);
                }
                finally { if (e_2) throw e_2.error; }
            }
            await bidirectionalStream.close();
        }
        catch (error) {
            if (typeof (bidirectionalStream === null || bidirectionalStream === void 0 ? void 0 : bidirectionalStream.cancel) === 'function') {
                try {
                    await bidirectionalStream.cancel();
                }
                catch (_cancelError) {
                    // Ignore cancellation errors to surface the original issue.
                }
            }
            if (error instanceof errors_1.STTError) {
                throw error;
            }
            throw new errors_1.STTError(`Failed to stream audio for recognition: ${error.message}`, error.stack);
        }
    }
    /**
     * Wraps protobuf response stream to surface writer errors.
     * @param {AsyncIterable<any>} pbStream - Bidirectional stream iterable
     * @param {Promise<void>} writerPromise - Promise tracking audio writer
     * @returns {AsyncIterable<any>} Wrapped stream that propagates writer errors
     * @private
     */
    wrapStreamWithWriter(pbStream, writerPromise) {
        let writerError = null;
        const monitoredWriter = writerPromise.catch((error) => {
            writerError =
                error instanceof errors_1.STTError
                    ? error
                    : new errors_1.STTError(`Failed to stream audio for recognition: ${error.message}`, error.stack);
            throw writerError;
        });
        return function () {
            return __asyncGenerator(this, arguments, function* () {
                var _a, e_3, _b, _c;
                try {
                    try {
                        for (var _d = true, pbStream_1 = __asyncValues(pbStream), pbStream_1_1; pbStream_1_1 = yield __await(pbStream_1.next()), _a = pbStream_1_1.done, !_a; _d = true) {
                            _c = pbStream_1_1.value;
                            _d = false;
                            const chunk = _c;
                            if (writerError) {
                                throw writerError;
                            }
                            yield yield __await(chunk);
                        }
                    }
                    catch (e_3_1) { e_3 = { error: e_3_1 }; }
                    finally {
                        try {
                            if (!_d && !_a && (_b = pbStream_1.return)) yield __await(_b.call(pbStream_1));
                        }
                        finally { if (e_3) throw e_3.error; }
                    }
                    yield __await(monitoredWriter);
                    if (writerError) {
                        throw writerError;
                    }
                }
                catch (error) {
                    if (writerError && error !== writerError) {
                        throw writerError;
                    }
                    throw error;
                }
            });
        }.call(this);
    }
    /**
     * Transforms protobuf text stream to string stream.
     * @param {AsyncIterable<any>} pbStream - Protobuf message stream
     * @returns {AsyncIterable<string>} Text stream
     * @private
     */
    transformSessionToText(session) {
        return __asyncGenerator(this, arguments, function* transformSessionToText_1() {
            var _a, e_4, _b, _c;
            try {
                for (var _d = true, session_1 = __asyncValues(session), session_1_1; session_1_1 = yield __await(session_1.next()), _a = session_1_1.done, !_a; _d = true) {
                    _c = session_1_1.value;
                    _d = false;
                    const result = _c;
                    if (result.text && result.text.trim().length > 0) {
                        yield yield __await(result.text);
                    }
                }
            }
            catch (e_4_1) { e_4 = { error: e_4_1 }; }
            finally {
                try {
                    if (!_d && !_a && (_b = session_1.return)) yield __await(_b.call(session_1));
                }
                finally { if (e_4) throw e_4.error; }
            }
        });
    }
    /**
     * Converts protobuf messages to structured transcription results.
     * @param {AsyncIterable<any>} pbStream - Protobuf message stream
     * @returns {AsyncIterable<StreamingSTTTranscription>} Result stream
     * @private
     */
    transformPbStreamToResult(pbStream) {
        return __asyncGenerator(this, arguments, function* transformPbStreamToResult_1() {
            var _a, e_5, _b, _c;
            try {
                try {
                    for (var _d = true, pbStream_2 = __asyncValues(pbStream), pbStream_2_1; pbStream_2_1 = yield __await(pbStream_2.next()), _a = pbStream_2_1.done, !_a; _d = true) {
                        _c = pbStream_2_1.value;
                        _d = false;
                        const pbMessage = _c;
                        const result = this.extractTranscriptionResult(pbMessage);
                        if (result.text && result.text.trim().length > 0) {
                            yield yield __await(result);
                        }
                    }
                }
                catch (e_5_1) { e_5 = { error: e_5_1 }; }
                finally {
                    try {
                        if (!_d && !_a && (_b = pbStream_2.return)) yield __await(_b.call(pbStream_2));
                    }
                    finally { if (e_5) throw e_5.error; }
                }
            }
            catch (error) {
                throw new errors_1.STTError(`Failed to process streaming STT results: ${error.message}`, error.stack);
            }
        });
    }
    /**
     * Extracts transcription result from a protobuf message.
     * @param {any} pbMessage - Protobuf message from the stream
     * @returns {StreamingSTTTranscription} Extracted transcription result
     * @private
     */
    extractTranscriptionResult(pbMessage) {
        var _a;
        try {
            const response = pb_helpers_1.PbHelper.Speech.unpackStreamSpeechRecognitionResponse(pbMessage);
            return {
                text: (_a = response === null || response === void 0 ? void 0 : response.text) !== null && _a !== void 0 ? _a : '',
                isFinal: Boolean(response === null || response === void 0 ? void 0 : response.isFinal),
            };
        }
        catch (streamingError) {
            try {
                const text = pb_helpers_1.PbHelper.Speech.unpackText(pbMessage);
                return { text, isFinal: true };
            }
            catch (_b) {
                throw new errors_1.STTError(`Failed to decode streaming STT response: ${streamingError instanceof Error
                    ? streamingError.message
                    : String(streamingError)}`, streamingError instanceof Error ? streamingError.stack : undefined);
            }
        }
    }
    async createStreamingSession(audioStream, options) {
        try {
            if (!audioStream) {
                throw new errors_1.STTError('Audio stream is required');
            }
            const pbAudioStream = this.transformAudioStreamToPb(audioStream);
            const request = pb_helpers_1.PbHelper.Speech.createStreamRecognizeSpeechRequest(options);
            const bidirectionalStream = await this.streamingSTTInterface.streamRecognizeSpeech(request);
            const writerPromise = this.pipeAudioStreamToBidirectionalStream(pbAudioStream, bidirectionalStream);
            const responseStream = this.wrapStreamWithWriter(bidirectionalStream, writerPromise);
            return { responseStream, bidirectionalStream };
        }
        catch (error) {
            if (error instanceof errors_1.STTError) {
                throw error;
            }
            throw new errors_1.STTError(`Failed to process streaming speech recognition: ${error.message}`, error.stack);
        }
    }
}
exports.StreamingSTT = StreamingSTT;
class StreamingSTTSessionImpl {
    constructor(resultStream, bidirectionalStream) {
        this.bidirectionalStream = bidirectionalStream;
        this.closed = false;
        this.finalSeen = false;
        this.iterator = resultStream[Symbol.asyncIterator]();
        this.responses = this;
    }
    [Symbol.asyncIterator]() {
        return this;
    }
    async next() {
        var _a;
        if (this.finalSeen) {
            await this.finalizeStream();
            return { done: true, value: undefined };
        }
        let result;
        try {
            result = await this.iterator.next();
        }
        catch (error) {
            await this.finalizeStream();
            throw error;
        }
        if (result.done) {
            await this.finalizeStream();
            return { done: true, value: undefined };
        }
        if ((_a = result.value) === null || _a === void 0 ? void 0 : _a.isFinal) {
            this.finalSeen = true;
            await this.finalizeStream();
        }
        return { done: false, value: result.value };
    }
    async return() {
        await this.finalizeStream();
        if (typeof this.iterator.return === 'function') {
            await this.iterator.return();
        }
        return { done: true, value: undefined };
    }
    async throw(error) {
        await this.finalizeStream();
        if (typeof this.iterator.throw === 'function') {
            await this.iterator.throw(error);
        }
        throw error;
    }
    getMetadata() {
        var _a, _b;
        if (typeof ((_a = this.bidirectionalStream) === null || _a === void 0 ? void 0 : _a.getMetadata) === 'function') {
            return (_b = this.bidirectionalStream.getMetadata()) !== null && _b !== void 0 ? _b : {};
        }
        return {};
    }
    async finalizeStream() {
        var _a, _b;
        if (this.closed) {
            return;
        }
        this.closed = true;
        try {
            if (typeof ((_a = this.bidirectionalStream) === null || _a === void 0 ? void 0 : _a.cancel) === 'function') {
                await this.bidirectionalStream.cancel();
            }
            else if (typeof ((_b = this.bidirectionalStream) === null || _b === void 0 ? void 0 : _b.close) === 'function') {
                await this.bidirectionalStream.close();
            }
        }
        catch (_error) {
            // Suppress cancellation errors to simplify API for consumers.
        }
    }
}
