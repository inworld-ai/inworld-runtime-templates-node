/**
 * Speech-to-Text (STT) primitive using N-API addon.
 */
import { AudioChunk, RecognizeSpeechConfig, STTCreationConfig } from './types';
/**
 * Speech-to-Text (STT) class.
 * Converts spoken audio into text transcriptions.
 * Supports both local models and cloud-based APIs.
 *
 * @example
 * ```typescript
 * // Create STT instance with cloud API
 * const stt = await STT.create({
 *   remoteConfig: {
 *     apiKey: process.env.INWORLD_API_KEY,
 *     defaultConfig: { languageCode: 'en-US' },
 *     defaultTimeout: { seconds: 30 }
 *   }
 * });
 *
 * // Recognize speech from audio
 * const stream = await stt.recognizeSpeech(audioChunk, { languageCode: 'en-US' });
 *
 * let transcription = '';
 * for await (const textChunk of stream) {
 *   transcription += textChunk;
 *   console.log('Partial:', textChunk);
 * }
 * console.log('Final:', transcription);
 * ```
 */
export declare class STT {
    private sttInterface;
    /**
     * Creates a new STT instance (internal constructor).
     * Use `STT.create()` static method instead.
     *
     * @param {any} sttInterface - Addon STTInterface instance
     * @internal
     */
    constructor(sttInterface: any);
    /**
     * Creates a new STT instance with the specified configuration.
     *
     * @param {STTCreationConfig} config - STT creation configuration
     * @returns {Promise<STT>} Promise resolving to STT instance
     * @throws {STTError} If STT creation fails
     *
     * @example
     * ```typescript
     * // Local model
     * const stt = await STT.create({
     *   localConfig: {
     *     modelPath: 'models/stt/whisper-base',
     *     device: { type: 'CPU', index: 0 },
     *     defaultConfig: { languageCode: 'en-US' }
     *   }
     * });
     *
     * // Cloud API
     * const stt = await STT.create({
     *   remoteConfig: {
     *     apiKey: process.env.INWORLD_API_KEY,
     *     defaultConfig: { languageCode: 'en-US' },
     *     defaultTimeout: { seconds: 30 }
     *   }
     * });
     * ```
     */
    static create(config: STTCreationConfig): Promise<STT>;
    /**
     * Recognizes speech from audio and returns a stream of text results.
     * The stream yields partial transcriptions as they become available.
     *
     * @param {AudioChunk} audio - Audio chunk to transcribe
     * @param {RecognizeSpeechConfig} config - Optional recognition configuration
     * @returns {Promise<AsyncIterable<string>>} Stream of text transcriptions
     * @throws {STTError} If speech recognition fails
     *
     * @example
     * ```typescript
     * const audioChunk = {
     *   data: audioData, // Float32Array
     *   sampleRate: 16000
     * };
     *
     * const stream = await stt.recognizeSpeech(audioChunk, {
     *   languageCode: 'en-US'
     * });
     *
     * let fullTranscription = '';
     * for await (const textChunk of stream) {
     *   fullTranscription += textChunk;
     *   console.log('Received:', textChunk);
     * }
     *
     * console.log('Complete transcription:', fullTranscription);
     * ```
     */
    recognizeSpeech(audio: AudioChunk, config?: RecognizeSpeechConfig): Promise<AsyncIterable<string>>;
    /**
     * Transforms protobuf stream to text stream.
     * @param {AsyncIterable<any>} pbStream - Protobuf message stream
     * @returns {AsyncIterable<string>} Text stream
     * @private
     */
    private transformPbStreamToText;
    /**
     * Convenience method to recognize speech and return complete transcription.
     * Waits for the entire stream to complete before returning.
     *
     * @param {AudioChunk} audio - Audio chunk to transcribe
     * @param {RecognizeSpeechConfig} config - Optional recognition configuration
     * @returns {Promise<string>} Complete transcription
     * @throws {STTError} If speech recognition fails
     *
     * @example
     * ```typescript
     * const audioChunk = {
     *   data: audioData,
     *   sampleRate: 16000
     * };
     *
     * const transcription = await stt.recognizeSpeechComplete(audioChunk);
     * console.log('Transcription:', transcription);
     * ```
     */
    recognizeSpeechComplete(audio: AudioChunk, config?: RecognizeSpeechConfig): Promise<string>;
}
