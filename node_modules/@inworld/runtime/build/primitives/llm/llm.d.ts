/**
 * Large Language Model (LLM) primitive using N-API addon.
 */
import type { LLMGenerationResult } from '../types';
import type { GenerateContentChatRequest, GenerateContentRequest, LLMCreationConfig } from './types';
/**
 * Large Language Model (LLM) class.
 * Generates text content using language models.
 * Supports both completion and chat modes, with streaming responses.
 *
 * @example
 * ```typescript
 * // Create LLM instance with cloud API
 * const llm = await LLM.create({
 *   remoteConfig: {
 *     provider: 'openai',
 *     modelName: 'gpt-4o-mini',
 *     apiKey: process.env.INWORLD_API_KEY,
 *     defaultConfig: {
 *       maxNewTokens: 100,
 *       temperature: 0.7
 *     }
 *   }
 * });
 *
 * // Generate content from prompt
 * const stream = await llm.generateContent({
 *   prompt: 'What is the capital of France?'
 * });
 *
 * for await (const result of stream) {
 *   console.log(result.content);
 * }
 * ```
 */
export declare class LLM {
    private llmInterface;
    /**
     * Creates a new LLM instance (internal constructor).
     * Use `LLM.create()` static method instead.
     *
     * @param {any} llmInterface - Addon LLMInterface instance
     * @internal
     */
    constructor(llmInterface: any);
    /**
     * Creates a new LLM instance with the specified configuration.
     *
     * @param {LLMCreationConfig} config - LLM creation configuration
     * @returns {Promise<LLM>} Promise resolving to LLM instance
     * @throws {LLMError} If LLM creation fails
     *
     * @example
     * ```typescript
     * // Local model
     * const llm = await LLM.create({
     *   localConfig: {
     *     modelPath: 'models/llm/llama3_1b',
     *     device: { type: 'CPU', index: 0 },
     *     defaultConfig: {
     *       maxNewTokens: 50,
     *       temperature: 0.7
     *     }
     *   }
     * });
     *
     * // Cloud API
     * const llm = await LLM.create({
     *   remoteConfig: {
     *     provider: 'openai',
     *     modelName: 'gpt-4o-mini',
     *     apiKey: process.env.INWORLD_API_KEY,
     *     defaultConfig: {
     *       maxNewTokens: 100,
     *       temperature: 0.7
     *     },
     *     defaultTimeout: { seconds: 30 }
     *   }
     * });
     * ```
     */
    static create(config: LLMCreationConfig): Promise<LLM>;
    /**
     * Generates content from a prompt (completion mode).
     * Returns a stream of generation results.
     *
     * @param {GenerateContentRequest} request - Generation request with prompt and config
     * @returns {Promise<AsyncIterable<LLMGenerationResult>>} Stream of generation results
     * @throws {LLMError} If content generation fails
     *
     * @example
     * ```typescript
     * const stream = await llm.generateContent({
     *   prompt: 'Write a haiku about programming',
     *   config: {
     *     maxNewTokens: 50,
     *     temperature: 0.8
     *   }
     * });
     *
     * let fullContent = '';
     * for await (const result of stream) {
     *   fullContent += result.content;
     *   console.log('Received:', result.content);
     * }
     *
     * console.log('Complete:', fullContent);
     * ```
     */
    generateContent(request: GenerateContentRequest): Promise<AsyncIterable<LLMGenerationResult>>;
    /**
     * Generates content from a chat conversation.
     * Supports multi-turn conversations, tool calling, and different response formats.
     *
     * @param {GenerateContentChatRequest} request - Chat generation request
     * @returns {Promise<AsyncIterable<LLMGenerationResult>>} Stream of generation results
     * @throws {LLMError} If chat generation fails
     *
     * @example
     * ```typescript
     * const stream = await llm.generateContentChat({
     *   messages: [
     *     { role: 'system', content: 'You are a helpful assistant.' },
     *     { role: 'user', content: 'What is the capital of France?' }
     *   ],
     *   config: {
     *     maxNewTokens: 100,
     *     temperature: 0.7
     *   }
     * });
     *
     * for await (const result of stream) {
     *   console.log('Assistant:', result.content);
     *
     *   // Handle tool calls if present
     *   if (result.toolCalls && result.toolCalls.length > 0) {
     *     for (const toolCall of result.toolCalls) {
     *       console.log('Tool call:', toolCall.name, toolCall.args);
     *     }
     *   }
     * }
     * ```
     */
    generateContentChat(request: GenerateContentChatRequest): Promise<AsyncIterable<LLMGenerationResult>>;
    /**
     * Transforms protobuf stream to LLMGenerationResult stream.
     * @param {AsyncIterable<any>} pbStream - Protobuf message stream
     * @returns {AsyncIterable<LLMGenerationResult>} Generation result stream
     * @private
     */
    private transformPbStreamToResults;
    /**
     * Convenience method to generate content and return complete result.
     * Waits for the entire stream to complete before returning.
     *
     * @param {GenerateContentRequest} request - Generation request
     * @returns {Promise<string>} Complete generated content
     * @throws {LLMError} If content generation fails
     *
     * @example
     * ```typescript
     * const content = await llm.generateContentComplete({
     *   prompt: 'Explain quantum computing in one paragraph'
     * });
     *
     * console.log(content);
     * ```
     */
    generateContentComplete(request: GenerateContentRequest): Promise<string>;
    /**
     * Convenience method to generate chat content and return complete result.
     * Waits for the entire stream to complete before returning.
     *
     * @param {GenerateContentChatRequest} request - Chat generation request
     * @returns {Promise<LLMGenerationResult>} Complete generation result with content and tool calls
     * @throws {LLMError} If chat generation fails
     *
     * @example
     * ```typescript
     * const result = await llm.generateContentChatComplete({
     *   messages: [
     *     { role: 'user', content: 'What is 2+2?' }
     *   ]
     * });
     *
     * console.log(result.content);
     * if (result.toolCalls) {
     *   console.log('Tool calls:', result.toolCalls);
     * }
     * ```
     */
    generateContentChatComplete(request: GenerateContentChatRequest): Promise<LLMGenerationResult>;
    /**
     * Simple chat interface for single-turn conversations.
     *
     * @param {string} message - User message
     * @param {string} systemPrompt - Optional system prompt
     * @returns {Promise<string>} Assistant's response
     * @throws {LLMError} If chat fails
     *
     * @example
     * ```typescript
     * const response = await llm.chat(
     *   'What is the meaning of life?',
     *   'You are a philosophical assistant.'
     * );
     *
     * console.log(response);
     * ```
     */
    chat(message: string, systemPrompt?: string): Promise<string>;
}
