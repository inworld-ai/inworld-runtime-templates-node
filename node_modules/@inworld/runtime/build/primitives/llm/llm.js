"use strict";
/**
 * Large Language Model (LLM) primitive using N-API addon.
 */
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __await = (this && this.__await) || function (v) { return this instanceof __await ? (this.v = v, this) : new __await(v); }
var __asyncGenerator = (this && this.__asyncGenerator) || function (thisArg, _arguments, generator) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var g = generator.apply(thisArg, _arguments || []), i, q = [];
    return i = Object.create((typeof AsyncIterator === "function" ? AsyncIterator : Object).prototype), verb("next"), verb("throw"), verb("return", awaitReturn), i[Symbol.asyncIterator] = function () { return this; }, i;
    function awaitReturn(f) { return function (v) { return Promise.resolve(v).then(f, reject); }; }
    function verb(n, f) { if (g[n]) { i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; if (f) i[n] = f(i[n]); } }
    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }
    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }
    function fulfill(value) { resume("next", value); }
    function reject(value) { resume("throw", value); }
    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLM = void 0;
const node_api_1 = require("../../internal/node_api");
const pb_helpers_1 = require("../../internal/pb_helpers");
const errors_1 = require("../errors");
/**
 * Large Language Model (LLM) class.
 * Generates text content using language models.
 * Supports both completion and chat modes, with streaming responses.
 *
 * @example
 * ```typescript
 * // Create LLM instance with cloud API
 * const llm = await LLM.create({
 *   remoteConfig: {
 *     provider: 'openai',
 *     modelName: 'gpt-4o-mini',
 *     apiKey: process.env.INWORLD_API_KEY,
 *     defaultConfig: {
 *       maxNewTokens: 100,
 *       temperature: 0.7
 *     }
 *   }
 * });
 *
 * // Generate content from prompt
 * const stream = await llm.generateContent({
 *   prompt: 'What is the capital of France?'
 * });
 *
 * for await (const result of stream) {
 *   console.log(result.content);
 * }
 * ```
 */
class LLM {
    /**
     * Creates a new LLM instance (internal constructor).
     * Use `LLM.create()` static method instead.
     *
     * @param {any} llmInterface - Addon LLMInterface instance
     * @internal
     */
    constructor(llmInterface) {
        this.llmInterface = llmInterface;
    }
    /**
     * Creates a new LLM instance with the specified configuration.
     *
     * @param {LLMCreationConfig} config - LLM creation configuration
     * @returns {Promise<LLM>} Promise resolving to LLM instance
     * @throws {LLMError} If LLM creation fails
     *
     * @example
     * ```typescript
     * // Local model
     * const llm = await LLM.create({
     *   localConfig: {
     *     modelPath: 'models/llm/llama3_1b',
     *     device: { type: 'CPU', index: 0 },
     *     defaultConfig: {
     *       maxNewTokens: 50,
     *       temperature: 0.7
     *     }
     *   }
     * });
     *
     * // Cloud API
     * const llm = await LLM.create({
     *   remoteConfig: {
     *     provider: 'openai',
     *     modelName: 'gpt-4o-mini',
     *     apiKey: process.env.INWORLD_API_KEY,
     *     defaultConfig: {
     *       maxNewTokens: 100,
     *       temperature: 0.7
     *     },
     *     defaultTimeout: { seconds: 30 }
     *   }
     * });
     * ```
     */
    static async create(config) {
        try {
            const env = (0, node_api_1.getInworldAddonEnv)();
            const llmNamespace = env.llm;
            if (!llmNamespace) {
                throw new errors_1.LLMError('LLM namespace not available in addon environment');
            }
            const factory = llmNamespace.createLLMFactory();
            const configPb = pb_helpers_1.PbHelper.LLM.createLLMCreationConfig(config);
            const llmInterface = await factory.createLLM(configPb);
            return new LLM(llmInterface);
        }
        catch (error) {
            if (error instanceof errors_1.LLMError) {
                throw error;
            }
            throw new errors_1.LLMError(`Failed to create LLM instance: ${error.message}`, error.stack);
        }
    }
    /**
     * Generates content from a prompt (completion mode).
     * Returns a stream of generation results.
     *
     * @param {GenerateContentRequest} request - Generation request with prompt and config
     * @returns {Promise<AsyncIterable<LLMGenerationResult>>} Stream of generation results
     * @throws {LLMError} If content generation fails
     *
     * @example
     * ```typescript
     * const stream = await llm.generateContent({
     *   prompt: 'Write a haiku about programming',
     *   config: {
     *     maxNewTokens: 50,
     *     temperature: 0.8
     *   }
     * });
     *
     * let fullContent = '';
     * for await (const result of stream) {
     *   fullContent += result.content;
     *   console.log('Received:', result.content);
     * }
     *
     * console.log('Complete:', fullContent);
     * ```
     */
    async generateContent(request) {
        try {
            if (!request) {
                throw new errors_1.LLMError('Generation request is required');
            }
            if (!request.prompt || request.prompt.trim().length === 0) {
                throw new errors_1.LLMError('Prompt cannot be empty');
            }
            const requestPb = pb_helpers_1.PbHelper.LLM.createGenerateContentRequest(request.prompt, request.config);
            // Returns an async iterable of protobuf LLMGenerationResult messages
            const pbStream = await this.llmInterface.generateContent(requestPb);
            // Transform the protobuf stream into LLMGenerationResult stream
            return this.transformPbStreamToResults(pbStream);
        }
        catch (error) {
            if (error instanceof errors_1.LLMError) {
                throw error;
            }
            throw new errors_1.LLMError(`Failed to generate content: ${error.message}`, error.stack);
        }
    }
    /**
     * Generates content from a chat conversation.
     * Supports multi-turn conversations, tool calling, and different response formats.
     *
     * @param {GenerateContentChatRequest} request - Chat generation request
     * @returns {Promise<AsyncIterable<LLMGenerationResult>>} Stream of generation results
     * @throws {LLMError} If chat generation fails
     *
     * @example
     * ```typescript
     * const stream = await llm.generateContentChat({
     *   messages: [
     *     { role: 'system', content: 'You are a helpful assistant.' },
     *     { role: 'user', content: 'What is the capital of France?' }
     *   ],
     *   config: {
     *     maxNewTokens: 100,
     *     temperature: 0.7
     *   }
     * });
     *
     * for await (const result of stream) {
     *   console.log('Assistant:', result.content);
     *
     *   // Handle tool calls if present
     *   if (result.toolCalls && result.toolCalls.length > 0) {
     *     for (const toolCall of result.toolCalls) {
     *       console.log('Tool call:', toolCall.name, toolCall.args);
     *     }
     *   }
     * }
     * ```
     */
    async generateContentChat(request) {
        try {
            if (!request) {
                throw new errors_1.LLMError('Chat request is required');
            }
            if (!request.messages || request.messages.length === 0) {
                throw new errors_1.LLMError('At least one message is required');
            }
            const requestPb = pb_helpers_1.PbHelper.LLM.createGenerateContentChatRequest(request.messages, {
                tools: request.tools,
                toolChoice: request.toolChoice,
                responseFormat: request.responseFormat,
                config: request.config,
            });
            // Returns an async iterable of protobuf LLMGenerationResult messages
            const pbStream = await this.llmInterface.generateContentChat(requestPb);
            // Transform the protobuf stream into LLMGenerationResult stream
            return this.transformPbStreamToResults(pbStream);
        }
        catch (error) {
            if (error instanceof errors_1.LLMError) {
                throw error;
            }
            throw new errors_1.LLMError(`Failed to generate chat content: ${error.message}`, error.stack);
        }
    }
    /**
     * Transforms protobuf stream to LLMGenerationResult stream.
     * @param {AsyncIterable<any>} pbStream - Protobuf message stream
     * @returns {AsyncIterable<LLMGenerationResult>} Generation result stream
     * @private
     */
    transformPbStreamToResults(pbStream) {
        return __asyncGenerator(this, arguments, function* transformPbStreamToResults_1() {
            var _a, e_1, _b, _c;
            try {
                try {
                    for (var _d = true, pbStream_1 = __asyncValues(pbStream), pbStream_1_1; pbStream_1_1 = yield __await(pbStream_1.next()), _a = pbStream_1_1.done, !_a; _d = true) {
                        _c = pbStream_1_1.value;
                        _d = false;
                        const pbMessage = _c;
                        const result = pb_helpers_1.PbHelper.LLM.unpackLLMGenerationResult(pbMessage);
                        // Null result indicates end of stream marker
                        if (result === null) {
                            continue;
                        }
                        if (result && result.content !== undefined) {
                            yield yield __await(result);
                        }
                    }
                }
                catch (e_1_1) { e_1 = { error: e_1_1 }; }
                finally {
                    try {
                        if (!_d && !_a && (_b = pbStream_1.return)) yield __await(_b.call(pbStream_1));
                    }
                    finally { if (e_1) throw e_1.error; }
                }
            }
            catch (error) {
                throw new errors_1.LLMError(`Failed to process LLM stream: ${error.message}`, error.stack);
            }
        });
    }
    /**
     * Convenience method to generate content and return complete result.
     * Waits for the entire stream to complete before returning.
     *
     * @param {GenerateContentRequest} request - Generation request
     * @returns {Promise<string>} Complete generated content
     * @throws {LLMError} If content generation fails
     *
     * @example
     * ```typescript
     * const content = await llm.generateContentComplete({
     *   prompt: 'Explain quantum computing in one paragraph'
     * });
     *
     * console.log(content);
     * ```
     */
    async generateContentComplete(request) {
        var _a, e_2, _b, _c;
        const stream = await this.generateContent(request);
        let fullContent = '';
        try {
            for (var _d = true, stream_1 = __asyncValues(stream), stream_1_1; stream_1_1 = await stream_1.next(), _a = stream_1_1.done, !_a; _d = true) {
                _c = stream_1_1.value;
                _d = false;
                const result = _c;
                fullContent += result.content || '';
            }
        }
        catch (e_2_1) { e_2 = { error: e_2_1 }; }
        finally {
            try {
                if (!_d && !_a && (_b = stream_1.return)) await _b.call(stream_1);
            }
            finally { if (e_2) throw e_2.error; }
        }
        return fullContent;
    }
    /**
     * Convenience method to generate chat content and return complete result.
     * Waits for the entire stream to complete before returning.
     *
     * @param {GenerateContentChatRequest} request - Chat generation request
     * @returns {Promise<LLMGenerationResult>} Complete generation result with content and tool calls
     * @throws {LLMError} If chat generation fails
     *
     * @example
     * ```typescript
     * const result = await llm.generateContentChatComplete({
     *   messages: [
     *     { role: 'user', content: 'What is 2+2?' }
     *   ]
     * });
     *
     * console.log(result.content);
     * if (result.toolCalls) {
     *   console.log('Tool calls:', result.toolCalls);
     * }
     * ```
     */
    async generateContentChatComplete(request) {
        var _a, e_3, _b, _c;
        const stream = await this.generateContentChat(request);
        let fullContent = '';
        const allToolCalls = [];
        try {
            for (var _d = true, stream_2 = __asyncValues(stream), stream_2_1; stream_2_1 = await stream_2.next(), _a = stream_2_1.done, !_a; _d = true) {
                _c = stream_2_1.value;
                _d = false;
                const result = _c;
                fullContent += result.content || '';
                if (result.toolCalls && result.toolCalls.length > 0) {
                    allToolCalls.push(...result.toolCalls);
                }
            }
        }
        catch (e_3_1) { e_3 = { error: e_3_1 }; }
        finally {
            try {
                if (!_d && !_a && (_b = stream_2.return)) await _b.call(stream_2);
            }
            finally { if (e_3) throw e_3.error; }
        }
        return {
            content: fullContent,
            stream: false,
            toolCalls: allToolCalls.length > 0 ? allToolCalls : undefined,
        };
    }
    /**
     * Simple chat interface for single-turn conversations.
     *
     * @param {string} message - User message
     * @param {string} systemPrompt - Optional system prompt
     * @returns {Promise<string>} Assistant's response
     * @throws {LLMError} If chat fails
     *
     * @example
     * ```typescript
     * const response = await llm.chat(
     *   'What is the meaning of life?',
     *   'You are a philosophical assistant.'
     * );
     *
     * console.log(response);
     * ```
     */
    async chat(message, systemPrompt) {
        const messages = [];
        if (systemPrompt) {
            messages.push({ role: 'system', content: systemPrompt });
        }
        messages.push({ role: 'user', content: message });
        const result = await this.generateContentChatComplete({ messages });
        return result.content;
    }
}
exports.LLM = LLM;
