import { ResponseFormat } from '../../../common/api/graphs';
import { Message as LLMChatRequestMessageClass, Tool as LLMChatRequestTool, ToolChoice as LLMChatRequestToolChoiceClass } from '../../../common/api/library';
import { LLMChatRequest } from '../../../common/data_types/api/llm_chat_request';
import { ContentStream } from '../../../common/stream';
import { ProcessContextPayload } from '../../../internal/types';
export type LLMInterface = {
    generateContent(input: string): Promise<ContentStream>;
    generateContent(input: LLMChatRequest): Promise<ContentStream>;
    generateContent(messages: LLMChatRequestMessageClass[], tools?: LLMChatRequestTool[], toolChoice?: LLMChatRequestToolChoiceClass, responseFormat?: ResponseFormat): Promise<ContentStream>;
};
export declare class LLMInterfaceWrapper implements LLMInterface {
    private llm;
    constructor(componentRegistry: ProcessContextPayload['componentRegistry'], componentId: string);
    generateContent(input: string): Promise<ContentStream>;
    generateContent(input: LLMChatRequest): Promise<ContentStream>;
    generateContent(messages: LLMChatRequestMessageClass[], tools?: LLMChatRequestTool[], toolChoice?: LLMChatRequestToolChoiceClass, responseFormat?: ResponseFormat): Promise<ContentStream>;
    private generateContentMessages;
}
