"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.RemoteLLMChatNode = void 0;
const default_1 = require("../../../internal/default");
const inworld_1 = require("../../../internal/error/inworld");
const remote_llm_component_1 = require("../components/remote_llm_component");
const snakify_1 = __importDefault(require("../utils/snakify"));
const abstract_node_1 = require("./abstract_node");
/**
 * Remote LLM chat node.
 * You can either use a pre-configured LLM component that could be reused across multiple nodes
 * or provide the LLM provider and model name and the node will create a new component for you.
 *
 * @input {LLMChatRequest} {@link GraphTypes.LLMChatRequest} - The data type that LLMChatNode accepts as input
 * @output {LLMChatResponse} {@link GraphTypes.ContentStream } | {@link GraphTypes.Content} - The data type that LLMChatNode outputs
 *
 * @example
 * ```typescript
 * // Using LLM provider configuration
 * const llmNode = new RemoteLLMChatNode({
 *   id: 'my-llm-node',
 *   provider: 'openai',
 *   modelName: 'gpt-4o-mini',
 *   stream: true
 * });
 *
 * // Using existing LLM component
 * const llmNodeWithComponent = new RemoteLLMChatNode({
 *   id: 'my-llm-node',
 *   llmComponent: existingLLMComponent
 * });
 *
 * // Using default settings
 * const defaultLlmNode = new RemoteLLMChatNode();
 * ```
 */
class RemoteLLMChatNode extends abstract_node_1.AbstractNode {
    /**
     * Creates a new RemoteLLMChatNode instance.
     *
     * @remarks
     * Provide LLM provider settings to create a new internal component, or pass
     * `llmComponent` to reuse an existing one. If omitted, defaults are applied.
     *
     * @param props - Optional configuration for the chat node.
     */
    constructor(props = {}) {
        var _a, _b, _c, _d, _e, _f;
        super(props);
        this.executionConfig = {
            llmComponentId: '',
            textGenerationConfig: Object.assign({}, (0, snakify_1.default)((_a = props.textGenerationConfig) !== null && _a !== void 0 ? _a : {})),
            stream: (_b = props.stream) !== null && _b !== void 0 ? _b : false,
            reportToClient: (_c = props.reportToClient) !== null && _c !== void 0 ? _c : false,
            responseFormat: (_d = props.responseFormat) !== null && _d !== void 0 ? _d : 'text',
            messageTemplates: props.messageTemplates,
        };
        let llmComponent;
        if ('llmComponent' in props) {
            llmComponent = props.llmComponent;
        }
        else {
            llmComponent = new remote_llm_component_1.RemoteLLMComponent({
                id: `${this.id}_llm_component`,
                provider: (_e = props.provider) !== null && _e !== void 0 ? _e : default_1.Default.llm.provider,
                modelName: (_f = props.modelName) !== null && _f !== void 0 ? _f : default_1.Default.llm.modelName,
            });
        }
        this.executionConfig.llmComponentId = llmComponent.id;
        this.addComponent(llmComponent);
    }
    toGraphConfigNode() {
        if (this.executionConfig.responseFormat === 'json' &&
            !this.executionConfig.messageTemplates) {
            throw new inworld_1.InworldError(`'JSON response format requires 'messageTemplates' to be defined in executionConfig.'`);
        }
        return {
            id: this.id,
            type: 'LLMChatNode',
            execution_config: {
                type: 'LLMChatNodeExecutionConfig',
                properties: (0, snakify_1.default)(this.executionConfig),
            },
        };
    }
}
exports.RemoteLLMChatNode = RemoteLLMChatNode;
