import { AbstractComponent } from '../components/abstract_component';
import { RemoteLLMComponent } from '../components/remote_llm_component';
import { Camelize } from '../constants';
import { MessageTemplate, Node as GraphConfigNode, TextGenerationConfig } from '../graph_config_schema';
import { AbstractNode, AbstractNodeProps } from './abstract_node';
/**
 * Configuration for `RemoteLLMChatNode` using LLM provider settings.
 *
 * @remarks
 * This approach creates both the node and the underlying LLM component. If
 * `provider`/`modelName` are omitted, sensible defaults will be used.
 */
export interface RemoteLLMChatNodeProps extends AbstractNodeProps {
    /** Text generation configuration parameters */
    textGenerationConfig?: Camelize<TextGenerationConfig>;
    /** Whether to stream responses */
    stream?: boolean;
    /** LLM provider (e.g., 'openai', 'anthropic', 'inworld') */
    provider?: string;
    /** Model name specific to the provider (e.g., 'gpt-4', 'claude-3-5-sonnet-20241022') */
    modelName?: string;
    responseFormat?: 'text' | 'json' | 'json_schema';
    messageTemplates?: Camelize<MessageTemplate>[];
}
/**
 * Configuration for `RemoteLLMChatNode` using an existing LLM component.
 *
 * @remarks
 * This approach references a pre-configured LLM component that can be reused
 * across multiple nodes.
 */
interface RemoteLLMChatNodeWithLLMComponentProps extends AbstractNodeProps {
    /** ID of the existing LLM component to use */
    llmComponent: RemoteLLMComponent | AbstractComponent;
    /** Text generation configuration parameters */
    textGenerationConfig?: Camelize<TextGenerationConfig>;
    /** Whether to stream responses */
    stream?: boolean;
    responseFormat: 'text' | 'json' | 'json_schema';
    messageTemplates?: Camelize<MessageTemplate>[];
}
/**
 * Remote LLM chat node.
 * You can either use a pre-configured LLM component that could be reused across multiple nodes
 * or provide the LLM provider and model name and the node will create a new component for you.
 *
 * @input {LLMChatRequest} {@link GraphTypes.LLMChatRequest} - The data type that LLMChatNode accepts as input
 * @output {LLMChatResponse} {@link GraphTypes.ContentStream } | {@link GraphTypes.Content} - The data type that LLMChatNode outputs
 *
 * @example
 * ```typescript
 * // Using LLM provider configuration
 * const llmNode = new RemoteLLMChatNode({
 *   id: 'my-llm-node',
 *   provider: 'openai',
 *   modelName: 'gpt-4o-mini',
 *   stream: true
 * });
 *
 * // Using existing LLM component
 * const llmNodeWithComponent = new RemoteLLMChatNode({
 *   id: 'my-llm-node',
 *   llmComponent: existingLLMComponent
 * });
 *
 * // Using default settings
 * const defaultLlmNode = new RemoteLLMChatNode();
 * ```
 */
export declare class RemoteLLMChatNode extends AbstractNode {
    private executionConfig;
    /**
     * Creates a new RemoteLLMChatNode instance.
     *
     * @remarks
     * Provide LLM provider settings to create a new internal component, or pass
     * `llmComponent` to reuse an existing one. If omitted, defaults are applied.
     *
     * @param props - Optional configuration for the chat node.
     */
    constructor(props?: RemoteLLMChatNodeProps | RemoteLLMChatNodeWithLLMComponentProps);
    protected toGraphConfigNode(): GraphConfigNode;
}
export {};
