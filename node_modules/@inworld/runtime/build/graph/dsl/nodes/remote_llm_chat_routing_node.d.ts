import { Camelize } from '../constants';
import { MessageTemplate, Node as GraphConfigNode, TextGenerationConfig } from '../graph_config_schema';
import { AbstractNode, AbstractNodeProps } from './abstract_node';
/**
 * Default configuration values for routing.
 */
export interface LLMChatRoutingDefaults {
    /** Default text generation configuration */
    textGenerationConfig?: Camelize<TextGenerationConfig>;
    /** Default message templates */
    messageTemplates?: Camelize<MessageTemplate>[];
}
/**
 * Routing strategy configuration.
 */
export interface LLMChatRoutingStrategy {
    [key: string]: unknown;
}
/**
 * Configuration for `RemoteLLMChatRoutingNode` with intelligent routing.
 *
 * @remarks
 * This node uses both creation_config (for API key and timeout) and
 * execution_config (for runtime settings). The API key is automatically
 * injected from the GraphBuilder or INWORLD_API_KEY env var. It provides
 * intelligent routing with automatic fallback to alternative models when the
 * primary model fails.
 */
export interface RemoteLLMChatRoutingNodeProps extends AbstractNodeProps {
    /** Default timeout for LLM requests in seconds (default: 30) */
    defaultTimeout?: number;
    /** Default configuration values used when routes don't specify their own */
    defaults?: LLMChatRoutingDefaults;
    /** Default routing strategy for this node */
    strategy?: LLMChatRoutingStrategy;
}
/**
 * Remote LLM chat routing node with intelligent fallback support.
 *
 * This node enables automatic routing and fallback to alternative LLM models
 * when the primary model fails, ensuring higher reliability for LLM-based
 * applications. Unlike RemoteLLMChatNode, this node handles model configuration
 * at runtime through the input LLMChatRoutingRequest.
 *
 * @input {LLMChatRoutingRequest} {@link GraphTypes.LLMChatRoutingRequest} - The
 * data type that LLMChatRoutingNode accepts as input
 * @output {LLMChatResponse} {@link GraphTypes.ContentStream } | {@link
 * GraphTypes.Content} - The data type that LLMChatRoutingNode outputs
 *
 * @example
 * ```typescript
 * // Create a routing node (API key is auto-injected)
 * const llmRoutingNode = new RemoteLLMChatRoutingNode({
 *   id: 'my-llm-routing-node',
 *   defaultTimeout: 30
 * });
 *
 * // The model configuration comes from the input request
 * const request = new GraphTypes.LLMChatRoutingRequest({
 *   messages: [...],
 *   modelId: { provider: 'groq', modelName: 'llama-3.1-70b-versatile' },
 *   routingConfig: {
 *     models: [
 *       { provider: 'openai', modelName: 'gpt-4o-mini' },
 *       { provider: 'openai', modelName: 'gpt-3.5-turbo' }
 *     ]
 *   },
 *   stream: true
 * });
 * ```
 */
export declare class RemoteLLMChatRoutingNode extends AbstractNode {
    private defaultTimeout?;
    private executionConfig;
    /**
     * Creates a new RemoteLLMChatRoutingNode instance.
     *
     * @param props - Configuration for the chat routing node.
     */
    constructor(props?: RemoteLLMChatRoutingNodeProps);
    protected toGraphConfigNode(): GraphConfigNode;
}
