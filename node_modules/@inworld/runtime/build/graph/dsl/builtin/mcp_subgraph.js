"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.mcpSubgraph = void 0;
const type_mapping_1 = require("../../../common/data_types/type_mapping");
const custom_node_1 = require("../nodes/custom_node");
const mcp_call_tool_node_1 = require("../nodes/mcp_call_tool_node");
const mcp_list_tools_node_1 = require("../nodes/mcp_list_tools_node");
const proxy_node_1 = require("../nodes/proxy_node");
const remote_llm_chat_node_1 = require("../nodes/remote_llm_chat_node");
const subgraph_builder_1 = require("../subgraph_builder");
/**
 * MCPServer is a class that represents an MCP server.
 * It is used to track the tools available to the MCP server.
 */
class MCPServer {
    constructor(id, mcpComponent) {
        this.tools = [];
        this.id = id;
        this.mcpComponent = mcpComponent;
    }
    extendTools(tools) {
        this.tools.push(...tools);
    }
    setListToolsNode(listToolsNode) {
        this.listToolsNode = listToolsNode;
    }
    setCallToolNode(callToolNode) {
        this.callToolNode = callToolNode;
    }
}
const mcpSubgraph = (parameters) => {
    var _a;
    const textGenerationConfig = (_a = parameters.textGenerationConfig) !== null && _a !== void 0 ? _a : {};
    const servers = parameters.mcpComponents.map((component) => new MCPServer(component.id, component));
    // Custom node to build LLM request from history messages and available tools
    class ToolsToLLMRequestNode extends custom_node_1.CustomNode {
        process(_context, historyMessages, ...listToolsResponses) {
            // Track tools for each server
            for (let i = 0; i < listToolsResponses.length; i++) {
                if (servers[i]) {
                    servers[i].extendTools(listToolsResponses[i].tools.map((tool) => tool.name));
                }
            }
            return new type_mapping_1.GraphTypes.LLMChatRequest({
                messages: historyMessages,
                tools: listToolsResponses.flatMap((response) => response.tools),
            });
        }
    }
    // Custom node to extract tool calls from LLM response
    class LLMResponseToToolCallsNode extends custom_node_1.CustomNode {
        process(_context, content) {
            return new type_mapping_1.GraphTypes.ToolCallRequest(content.toolCalls || []);
        }
    }
    // Custom node to handle the case where no tool calls are made.
    class NoopToolCallsNode extends custom_node_1.CustomNode {
        process(_context, _toolCallRequest) {
            return new type_mapping_1.GraphTypes.ToolCallResponse({ toolCallResults: [] });
        }
    }
    // Custom node to combine tool results with history messages for final LLM call
    // This node is optional and can be combined with history\ToLLMRequestNode
    // if conversation history is not needed for future subgraph calls.
    class ToolResultsToHistoryNode extends custom_node_1.CustomNode {
        constructor(toolCallId = 'ToolResultsToHistoryNode') {
            super();
            this.toolCallId = toolCallId;
        }
        process(_context, historyMessages, firstLLMResponse, ...toolCallResponses) {
            const messages = [
                ...historyMessages,
                {
                    role: 'assistant',
                    content: firstLLMResponse.content,
                    toolCalls: firstLLMResponse.toolCalls,
                    toolCallId: this.toolCallId,
                },
            ];
            // Add tool results
            for (const toolCallResponse of toolCallResponses) {
                if (toolCallResponse && toolCallResponse.toolCallResults.length > 0) {
                    for (const toolCallResult of toolCallResponse.toolCallResults) {
                        messages.push({
                            role: 'tool',
                            content: toolCallResult.result,
                            toolCallId: '', //FIXME
                        });
                    }
                    break;
                }
            }
            return messages;
        }
    }
    // Custom node to build final LLM request from messages that include tool calls and tool results
    class HistoryToLLMRequestNode extends custom_node_1.CustomNode {
        process(_context, messages) {
            return new type_mapping_1.GraphTypes.LLMChatRequest({ messages });
        }
    }
    // Custom node to process and return final results
    class MCPResultNode extends custom_node_1.CustomNode {
        process(_context, messages, toolResultResponse, toolCallId = 'MCPResultNode') {
            if (toolResultResponse) {
                return [
                    ...messages,
                    {
                        role: 'assistant',
                        content: (toolResultResponse === null || toolResultResponse === void 0 ? void 0 : toolResultResponse.content) || '',
                        toolCalls: (toolResultResponse === null || toolResultResponse === void 0 ? void 0 : toolResultResponse.toolCalls) || [],
                        toolCallId: toolCallId,
                    },
                ];
            }
            return messages;
        }
    }
    // Create MCP nodes for each server
    servers.forEach((server) => {
        server.setListToolsNode(new mcp_list_tools_node_1.MCPListToolsNode({
            id: `${server.id}_list_tools_node`,
            mcpComponent: server.mcpComponent,
        }));
        server.setCallToolNode(new mcp_call_tool_node_1.MCPCallToolNode({
            id: `${server.id}_call_tool_node`,
            mcpComponent: server.mcpComponent,
        }));
    });
    // Proxy node to route input to both the LLM nodes
    const inputNode = new proxy_node_1.ProxyNode({
        id: 'input_node',
    });
    const toolsToLLMRequestNode = new ToolsToLLMRequestNode();
    const initialLLMNode = new remote_llm_chat_node_1.RemoteLLMChatNode({
        id: 'initial_llm_node',
        llmComponent: parameters.llmComponent,
        textGenerationConfig: textGenerationConfig,
    });
    const llmResponseToToolCallsNode = new LLMResponseToToolCallsNode();
    const noopToolCallsNode = new NoopToolCallsNode();
    const toolResultsToHistoryNode = new ToolResultsToHistoryNode();
    const historyToLLMRequestNode = new HistoryToLLMRequestNode();
    const finalLLMNode = new remote_llm_chat_node_1.RemoteLLMChatNode({
        id: 'final_llm_node',
        llmComponent: parameters.llmComponent,
        textGenerationConfig: textGenerationConfig,
    });
    const resultNode = new MCPResultNode();
    let subgraphBuilder = new subgraph_builder_1.SubgraphBuilder('mcp_subgraph')
        .addNode(inputNode)
        .addNode(toolsToLLMRequestNode)
        .addNode(initialLLMNode)
        .addNode(llmResponseToToolCallsNode)
        .addNode(toolResultsToHistoryNode)
        .addNode(noopToolCallsNode)
        .addNode(historyToLLMRequestNode)
        .addNode(finalLLMNode)
        .addNode(resultNode);
    for (const server of servers) {
        subgraphBuilder = subgraphBuilder
            .addNode(server.listToolsNode)
            .addNode(server.callToolNode);
    }
    subgraphBuilder = subgraphBuilder
        // Input flows to tools listing and request building
        .addEdge(inputNode, toolsToLLMRequestNode)
        // Tools listing goes to initial LLM
        .addEdge(toolsToLLMRequestNode, initialLLMNode)
        // LLM response goes to tool call extractor
        .addEdge(initialLLMNode, llmResponseToToolCallsNode)
        // Original query and LLM response flow to result combiner
        .addEdge(inputNode, toolResultsToHistoryNode)
        .addEdge(initialLLMNode, toolResultsToHistoryNode)
        .addEdge(llmResponseToToolCallsNode, noopToolCallsNode)
        .addEdge(noopToolCallsNode, toolResultsToHistoryNode)
        .addEdge(toolResultsToHistoryNode, historyToLLMRequestNode, {
        conditionRef: (input) => {
            var _a;
            return ((_a = input.at(-1)) === null || _a === void 0 ? void 0 : _a.role) === 'tool';
        },
        optional: true,
    })
        .addEdge(historyToLLMRequestNode, finalLLMNode)
        // Updated history and llm response go to result node
        .addEdge(toolResultsToHistoryNode, resultNode)
        .addEdge(finalLLMNode, resultNode, { optional: true });
    // Add edges for MCP servers. the order must be the same as the order of the servers in the servers array.
    for (const server of servers) {
        subgraphBuilder = subgraphBuilder
            // Input flows to tools listing to trigger them to start
            .addEdge(inputNode, server.listToolsNode)
            // All list tools nodes feed into the request builder
            .addEdge(server.listToolsNode, toolsToLLMRequestNode)
            // Tool calls get routed to appropriate MCP servers (conditional edge depending on the tool called)
            .addEdge(llmResponseToToolCallsNode, server.callToolNode, {
            conditionRef: (input) => {
                return input.toolCalls.some((toolCall) => server.tools.includes(toolCall.name));
            },
            optional: true,
        })
            // Tool results flow to combiner (optional edge in case no tool calls are made)
            .addEdge(server.callToolNode, toolResultsToHistoryNode, {
            optional: true,
        });
    }
    return subgraphBuilder.setStartNode(inputNode).setEndNode(resultNode);
};
exports.mcpSubgraph = mcpSubgraph;
