import { PbBaseType } from '../../../internal/pb';
import { TaggedType } from '../../../internal/types';
import { LLMChatMetadata, LLMChatResponse as LLMChatResponseInterface, ToolCallInterface, Usage } from '../common';
import { AbstractApiDataType } from './abstract_api_data_type';
/**
 * Represents a complete LLM chat response with optional routing metadata.
 *
 * This class mirrors the schema defined in `LLMChatResponse` so it can be
 * validated, serialized to protobuf, and passed through the graph runtime just
 * like other API data types.
 */
export declare class LLMChatResponse extends AbstractApiDataType implements LLMChatResponseInterface {
    /**
     * Generated text content (non-streaming case).
     */
    readonly content?: string;
    /**
     * Finish reason reported by the model.
     */
    readonly finishReason?: LLMChatResponseInterface['finishReason'];
    /**
     * Routing metadata with attempt details and reasoning.
     * Only populated when using LLMChatRoutingNode.
     */
    readonly metadata?: LLMChatMetadata;
    /**
     * Identifier of the model that produced the response.
     */
    readonly modelName?: string;
    /**
     * Indicates whether the response was streamed.
     */
    readonly stream?: boolean;
    /**
     * Tool calls issued by the model.
     */
    readonly toolCalls?: ToolCallInterface[];
    /**
     * Token usage metadata.
     */
    readonly usage?: Usage;
    /**
     * Creates a new LLMChatResponse instance.
     * @param {LLMChatResponseInterface} llmChatResponse - Response payload to wrap.
     */
    constructor(llmChatResponse: LLMChatResponseInterface);
    /** @internal */
    toTaggedValue(): TaggedType;
    /** @internal */
    toProto(): PbBaseType;
}
