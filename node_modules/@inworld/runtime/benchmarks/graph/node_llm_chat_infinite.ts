import 'dotenv/config';

import {
  GraphBuilder,
  GraphTypes,
  RemoteLLMChatNode,
} from '@inworld/runtime/graph';

import {
  DEFAULT_LLM_MODEL_NAME,
  DEFAULT_LLM_PROVIDER,
  TOOLS,
} from './constants';

const minimist = require('minimist');

const usage = `
Usage:
    yarn node-llm-chat-infinite "Tell me the weather in Vancouver and evaluate the expression 2 + 2" \n
    --modelName=<model-name>[optional, default=${DEFAULT_LLM_MODEL_NAME}] \n
    --provider=<service-provider>[optional, default=${DEFAULT_LLM_PROVIDER}] \n
    --stream=<true|false>[optional, default=true, enable/disable streaming] \n
    --tools[optional, enable tool calling demonstration] \n
    --toolChoice=<auto|required|none|function_name>[optional, tool choice strategy when --tools is used] \n
    --imageUrl=<image-url>[optional, include an image in the message for multimodal input]
    --responseFormat=<text|json>[optional, response format for the LLM]
    --toolCallHistory=<true|false>[optional, enable/disable tool call history]
    --sleepSeconds=<number>[optional, default=20, seconds to sleep between iterations]

Examples:
    # Basic infinite loop request
    yarn node-llm-chat-infinite "Tell me the weather in Vancouver"

    # Basic infinite loop request with tools and custom sleep time
    yarn node-llm-chat-infinite "What is 15 + 27?" --modelName="gpt-4o-mini" --provider="openai" --tools --toolChoice="auto" --sleepSeconds=10
    
    # Specific tool choice with 30 second intervals
    yarn node-llm-chat-infinite "What is the weather in Vancouver?" --modelName="gpt-4o-mini" --provider="openai" --tools --toolChoice="get_weather" --sleepSeconds=30
    
    # Multimodal request with image
    yarn node-llm-chat-infinite "What do you see in this image?" --modelName="gpt-4o" --provider="openai" --imageUrl="https://upload.wikimedia.org/wikipedia/en/a/a9/Example.jpg"

    # Request with response format
    yarn node-llm-chat-infinite "Generate a user profile for a software engineer. Include name, profession, experience_years, skills array, and location. return in json format" --modelName="gpt-4o-mini" --provider="openai" --responseFormat="json"
    `;

// Memory tracking variables
interface MemoryStats {
  rss: number;
  heapUsed: number;
  heapTotal: number;
  external: number;
}

interface MemoryTracker {
  iterations: MemoryStats[];
  startTime: Date;
}

const memoryTracker: MemoryTracker = {
  iterations: [],
  startTime: new Date(),
};

// Signal handler for graceful shutdown
process.on('SIGINT', () => {
  console.log('\n\nüõë Received Ctrl+C, shutting down gracefully...');
  displayMemorySummary();
  process.exit(0);
});

run();

async function run() {
  const {
    prompt,
    modelName,
    provider,
    apiKey,
    tools,
    stream,
    toolChoice,
    imageUrl,
    responseFormat,
    toolCallHistory,
    sleepSeconds,
  } = parseArgs();

  console.log(
    `üöÄ Starting infinite loop LLM chat with ${sleepSeconds}s intervals`,
  );
  console.log('Press Ctrl+C to stop\n');

  let iteration = 1;

  while (true) {
    try {
      console.log(`\nüîÑ Iteration ${iteration} - ${new Date().toISOString()}`);
      console.log('='.repeat(60));

      const llmNode = new RemoteLLMChatNode({
        stream,
        provider,
        modelName,
        textGenerationConfig: {
          maxNewTokens: 200,
        },
      });

      const graph = new GraphBuilder({
        id: `node_llm_chat_infinite_graph_${iteration}`,
        enableRemoteConfig: false,
        apiKey,
      })
        .addNode(llmNode)
        .setStartNode(llmNode)
        .setEndNode(llmNode)
        .build();

      let graphInput;

      if (tools) {
        graphInput = createMessagesWithTools(
          prompt,
          toolChoice,
          imageUrl,
          toolCallHistory,
        );
      } else {
        graphInput = createMessages(prompt, imageUrl, toolCallHistory);
      }

      if (responseFormat) {
        graphInput.responseFormat = responseFormat;
      }

      console.log('Graph Input:', graphInput);

      const { outputStream } = graph.start(
        new GraphTypes.LLMChatRequest(graphInput),
      );

      for await (const result of outputStream) {
        await result.processResponse({
          Content: (response: GraphTypes.Content) => {
            console.log('üì• LLM Chat Response:');
            console.log('  Content:', response.content);
            if (response.toolCalls && response.toolCalls.length > 0) {
              console.log('  Tool Calls:');
              response.toolCalls.forEach((toolCall, index) => {
                console.log(
                  `    ${index + 1}. ${toolCall.name}(${toolCall.args})`,
                );
                console.log(`       ID: ${toolCall.id}`);
              });
            }
          },
          ContentStream: async (stream: GraphTypes.ContentStream) => {
            console.log('üì° LLM Chat Response Stream:');
            let streamContent = '';
            const toolCalls: { [id: string]: any } = {};
            let chunkCount = 0;
            for await (const chunk of stream) {
              chunkCount++;
              if (chunk.text) {
                streamContent += chunk.text;
                process.stdout.write(chunk.text);
              }
              if (chunk.toolCalls && chunk.toolCalls.length > 0) {
                for (const toolCall of chunk.toolCalls) {
                  if (toolCalls[toolCall.id]) {
                    toolCalls[toolCall.id].args += toolCall.args;
                  } else {
                    toolCalls[toolCall.id] = { ...toolCall };
                  }
                }
              }
            }
            console.log(`\nTotal chunks: ${chunkCount}`);
            console.log(
              `Final content length: ${streamContent.length} characters`,
            );
            const finalToolCalls = Object.values(toolCalls);
            if (finalToolCalls.length > 0) {
              console.log('Tool Calls from Stream:');
              finalToolCalls.forEach((toolCall, index) => {
                console.log(
                  `  ${index + 1}. ${toolCall.name}(${toolCall.args})`,
                );
                console.log(`     ID: ${toolCall.id}`);
              });
            }
          },
          default: (data: any) => {
            console.error('Unprocessed response:', data);
          },
        });
      }

      console.log(`\n‚úÖ Iteration ${iteration} completed`);

      // Collect and report memory usage
      const memoryUsage = process.memoryUsage();
      memoryTracker.iterations.push({
        rss: memoryUsage.rss,
        heapUsed: memoryUsage.heapUsed,
        heapTotal: memoryUsage.heapTotal,
        external: memoryUsage.external,
      });

      reportMemoryUsage(memoryUsage, iteration);

      // Sleep for specified seconds before next iteration
      if (sleepSeconds > 0) {
        console.log(`üò¥ Sleeping for ${sleepSeconds} seconds...`);
        await sleep(sleepSeconds * 1000);
      }

      iteration++;
    } catch (error) {
      console.error(`‚ùå Error in iteration ${iteration}:`, error);

      // Still collect memory usage even on error
      const memoryUsage = process.memoryUsage();
      memoryTracker.iterations.push({
        rss: memoryUsage.rss,
        heapUsed: memoryUsage.heapUsed,
        heapTotal: memoryUsage.heapTotal,
        external: memoryUsage.external,
      });

      reportMemoryUsage(memoryUsage, iteration);

      console.log(
        `üîÑ Continuing to next iteration after ${sleepSeconds} seconds...`,
      );
      await sleep(sleepSeconds * 1000);
      iteration++;
    }
  }
}

function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

function formatBytes(bytes: number): string {
  const mb = bytes / (1024 * 1024);
  if (mb >= 1024) {
    return `${(mb / 1024).toFixed(2)} GB`;
  }
  return `${mb.toFixed(2)} MB`;
}

function reportMemoryUsage(
  memoryUsage: NodeJS.MemoryUsage,
  iteration: number,
): void {
  console.log(`üìä Memory Usage (Iteration ${iteration}):`);
  console.log(`   RSS: ${formatBytes(memoryUsage.rss)} (Resident Set Size)`);
  console.log(`   Heap Used: ${formatBytes(memoryUsage.heapUsed)}`);
  console.log(`   Heap Total: ${formatBytes(memoryUsage.heapTotal)}`);
  console.log(`   External: ${formatBytes(memoryUsage.external)}`);
}

function displayMemorySummary(): void {
  if (memoryTracker.iterations.length === 0) {
    console.log('üìä No memory data collected.');
    return;
  }

  const endTime = new Date();
  const totalDuration = endTime.getTime() - memoryTracker.startTime.getTime();
  const durationMinutes = Math.floor(totalDuration / 60000);
  const durationSeconds = Math.floor((totalDuration % 60000) / 1000);

  console.log('\nüìä Memory Usage Summary:');
  console.log('='.repeat(50));
  console.log(`Total Runtime: ${durationMinutes}m ${durationSeconds}s`);
  console.log(`Total Iterations: ${memoryTracker.iterations.length}`);

  // Calculate statistics for each memory metric
  const metrics = ['rss', 'heapUsed', 'heapTotal', 'external'] as const;

  for (const metric of metrics) {
    const values = memoryTracker.iterations.map((iter) => iter[metric]);
    const min = Math.min(...values);
    const max = Math.max(...values);
    const avg = values.reduce((sum, val) => sum + val, 0) / values.length;

    const metricName =
      metric === 'rss'
        ? 'RSS'
        : metric === 'heapUsed'
          ? 'Heap Used'
          : metric === 'heapTotal'
            ? 'Heap Total'
            : 'External';

    console.log(`\n${metricName}:`);
    console.log(`   Min:  ${formatBytes(min)}`);
    console.log(`   Max:  ${formatBytes(max)}`);
    console.log(`   Avg:  ${formatBytes(avg)}`);
    console.log(`   Growth: ${formatBytes(max - min)}`);
  }

  // Memory trend analysis
  if (memoryTracker.iterations.length > 1) {
    const firstIteration = memoryTracker.iterations[0];
    const lastIteration =
      memoryTracker.iterations[memoryTracker.iterations.length - 1];

    console.log('\nüìà Memory Trend (First vs Last):');
    console.log(
      `   RSS Growth: ${formatBytes(lastIteration.rss - firstIteration.rss)}`,
    );
    console.log(
      `   Heap Used Growth: ${formatBytes(lastIteration.heapUsed - firstIteration.heapUsed)}`,
    );
    console.log(
      `   Heap Total Growth: ${formatBytes(lastIteration.heapTotal - firstIteration.heapTotal)}`,
    );
    console.log(
      `   External Growth: ${formatBytes(lastIteration.external - firstIteration.external)}`,
    );
  }

  console.log('\nüèÅ Application terminated.');
}

function createMessages(
  prompt: string,
  imageUrl?: string,
  toolCallHistory?: boolean,
) {
  const systemMessage = {
    role: 'system',
    content:
      'You are a helpful assistant that can use tools when needed. When analyzing images, describe what you see and use appropriate tools if calculations or weather information is needed.',
  };

  const previousUserMessage = {
    role: 'user',
    content: 'Hi please call the calculator tool to calculate 2 + 2',
  };

  const firstAssistantMessage = {
    role: 'assistant',
    content: '',
    toolCalls: [
      {
        id: '1',
        name: 'calculator',
        args: '{"a": 2, "b": 2}',
      },
    ],
  };

  const toolMessage = {
    role: 'tool',
    toolCallId: '1',
    content: '5',
  };

  let userMessage;
  if (imageUrl) {
    console.log('imageUrl', imageUrl);
    userMessage = {
      role: 'user',
      content: [
        {
          type: 'text' as const,
          text: prompt,
        },
        {
          type: 'image' as const,
          image_url: {
            url: imageUrl,
            detail: 'high',
          },
        },
      ],
    };
  } else {
    userMessage = {
      role: 'user',
      content: prompt,
    };
  }

  if (toolCallHistory) {
    return {
      messages: [
        systemMessage,
        previousUserMessage,
        firstAssistantMessage,
        toolMessage,
        userMessage,
      ],
    };
  } else {
    return {
      messages: [systemMessage, userMessage],
    };
  }
}

function createMessagesWithTools(
  userPrompt: string,
  toolChoice?: string,
  imageUrl?: string,
  toolCallHistory?: boolean,
) {
  const messages = createMessages(
    userPrompt,
    imageUrl,
    toolCallHistory,
  ).messages;

  const result: any = {
    messages,
    tools: TOOLS.map(normalizeToolDefinition),
  };

  if (toolChoice) {
    if (
      toolChoice === 'auto' ||
      toolChoice === 'required' ||
      toolChoice === 'none'
    ) {
      result.toolChoice = {
        choice: toolChoice,
      };
    } else {
      // Assume it's a specific function name
      result.toolChoice = {
        choice: {
          name: toolChoice,
        },
      };
    }
  }

  return result;
}

function normalizeToolDefinition(tool: any) {
  if (!tool) {
    return tool;
  }
  if (tool.properties !== undefined && typeof tool.properties !== 'string') {
    return {
      ...tool,
      properties: JSON.stringify(tool.properties),
    };
  }
  return tool;
}

function parseArgs(): {
  prompt: string;
  modelName: string;
  provider: string;
  apiKey: string;
  tools: boolean;
  stream: boolean;
  toolChoice?: string;
  imageUrl?: string;
  responseFormat?: string;
  toolCallHistory?: boolean;
  sleepSeconds: number;
} {
  const argv = minimist(process.argv.slice(2));

  if (argv.help) {
    console.log(usage);
    process.exit(0);
  }
  const prompt = argv._?.join(' ') || '';
  const modelName = argv.modelName || DEFAULT_LLM_MODEL_NAME;
  const provider = argv.provider || DEFAULT_LLM_PROVIDER;
  const apiKey = process.env.INWORLD_API_KEY || '';
  const tools = !!argv.tools;
  const stream = argv.stream !== undefined ? argv.stream === 'true' : true;
  const toolChoice = argv.toolChoice || undefined;
  const imageUrl = argv.imageUrl || undefined;
  const responseFormat = argv.responseFormat || undefined;
  const toolCallHistory =
    argv.toolCallHistory !== undefined
      ? argv.toolCallHistory === 'true'
      : false;
  const sleepSeconds = parseInt(argv.sleepSeconds) || 20;

  if (!prompt) {
    throw new Error(`You need to provide a prompt.\n${usage}`);
  }

  if (!apiKey) {
    throw new Error(
      `You need to set INWORLD_API_KEY environment variable.\n${usage}`,
    );
  }

  return {
    prompt,
    modelName,
    provider,
    apiKey,
    tools,
    stream,
    toolChoice,
    imageUrl,
    responseFormat,
    toolCallHistory,
    sleepSeconds,
  };
}
